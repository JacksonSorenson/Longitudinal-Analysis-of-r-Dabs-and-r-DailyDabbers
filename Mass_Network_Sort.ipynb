{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Threading: 100%|██████████| 4284/4284 [00:00<00:00, 13659.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Thread network saved → /Users/jacksonsorenson/Desktop/r_dailydabbers_network.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "DailyDabbers post + comment network CSV\n",
    "post ▸ comment (depth 1) ▸ reply (depth ≥2)  …\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── 1.  FILES ────────────────────────────────────────────────────\n",
    "POSTS_FILE = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:DailyDabbers/Extracted Coulumns/\"\n",
    "    \"r_daily_dabbers_posts_subset.csv\"\n",
    ")\n",
    "COMMENTS_FILE = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:DailyDabbers/Extracted Coulumns/\"\n",
    "    \"r_dailydabbers_comments_subset.csv\"\n",
    ")\n",
    "OUT_FILE = Path.home() / \"Desktop\" / \"r_dailydabbers_network.csv\"\n",
    "\n",
    "# ── 2.  HELPERS ─────────────────────────────────────────────────\n",
    "def strip_prefix(x: str) -> str:\n",
    "    return x.split(\"_\", 1)[1] if isinstance(x, str) and \"_\" in x else x\n",
    "def to_int(x):\n",
    "    try: return int(x)\n",
    "    except Exception: return pd.NA\n",
    "\n",
    "# ── 3.  LOAD + NORMALISE ───────────────────────────────────────\n",
    "posts    = pd.read_csv(POSTS_FILE, dtype=str)\n",
    "comments = pd.read_csv(COMMENTS_FILE, dtype=str)\n",
    "\n",
    "posts[\"post_id\"]       = posts[\"id\"].apply(strip_prefix)\n",
    "posts[\"created_utc_i\"] = posts[\"created_utc\"].apply(to_int)\n",
    "\n",
    "comments[\"comment_id\"]     = comments[\"id\"].apply(strip_prefix)\n",
    "comments[\"parent_id_norm\"] = comments[\"parent_id\"].apply(strip_prefix)\n",
    "comments[\"created_utc_i\"]  = comments[\"created_utc\"].apply(to_int)\n",
    "\n",
    "# ── 4.  CHILD LISTS  ────────────────────────────────────────────\n",
    "children = {}\n",
    "for _, row in comments.iterrows():\n",
    "    children.setdefault(row[\"parent_id_norm\"], []).append(row)\n",
    "for lst in children.values():\n",
    "    lst.sort(key=lambda r: r[\"created_utc_i\"] or 0)\n",
    "\n",
    "# ── 5.  TEXT LOOK-UPS  ─────────────────────────────────────────\n",
    "post_text   = {r[\"post_id\"]: f\"{r.get('title','')}\\n\\n{r.get('selftext','')}\"\n",
    "               for _, r in posts.iterrows()}\n",
    "comment_text = {r[\"comment_id\"]: r.get(\"body\", \"\")\n",
    "                for _, r in comments.iterrows()}\n",
    "parent_text  = lambda pid: comment_text.get(pid) or post_text.get(pid) or \"\"\n",
    "\n",
    "# ── 6.  DEPTH-FIRST WALK  ──────────────────────────────────────\n",
    "rows_out = []\n",
    "\n",
    "def dfs(parent_id: str, depth: int):\n",
    "    for com in children.get(parent_id, []):\n",
    "        cid  = com[\"comment_id\"]\n",
    "        node = \"comment\" if depth == 1 else \"reply\"   # ← ★ key change\n",
    "        rows_out.append({\n",
    "            \"node_type\"        : node,\n",
    "            \"depth\"            : depth,\n",
    "            \"id\"               : cid,\n",
    "            \"parent_id\"        : com[\"parent_id_norm\"],\n",
    "            \"post_id\"          : com[\"link_id\"].split(\"_\",1)[1] if \"link_id\" in com else \"\",\n",
    "            \"created_utc\"      : com[\"created_utc\"],\n",
    "            \"author\"           : com[\"author\"],\n",
    "            \"score\"            : com[\"score\"],\n",
    "            \"body_text\"        : comment_text[cid],\n",
    "            \"parent_body_text\" : parent_text(com[\"parent_id_norm\"]),\n",
    "            \"distinguished\"    : com.get(\"distinguished\", \"\")\n",
    "        })\n",
    "        dfs(cid, depth + 1)\n",
    "\n",
    "posts = posts.sort_values(\"created_utc_i\")\n",
    "for _, p in tqdm(posts.iterrows(), total=len(posts), desc=\"Threading\"):\n",
    "    pid = p[\"post_id\"]\n",
    "    rows_out.append({\n",
    "        \"node_type\"        : \"post\",\n",
    "        \"depth\"            : 0,\n",
    "        \"id\"               : pid,\n",
    "        \"parent_id\"        : \"\",\n",
    "        \"post_id\"          : pid,\n",
    "        \"created_utc\"      : p[\"created_utc\"],\n",
    "        \"author\"           : p[\"author\"],\n",
    "        \"score\"            : p[\"score\"],\n",
    "        \"body_text\"        : post_text[pid],\n",
    "        \"parent_body_text\" : \"\",\n",
    "        \"distinguished\"    : p.get(\"distinguished\", \"\"),\n",
    "        \"num_comments\"     : p.get(\"num_comments\", \"\"),\n",
    "        \"media\"            : p.get(\"media\", \"\")\n",
    "    })\n",
    "    dfs(pid, 1)\n",
    "\n",
    "# ── 7.  SAVE  ───────────────────────────────────────────────────\n",
    "pd.DataFrame(rows_out).to_csv(OUT_FILE, index=False)\n",
    "print(f\"✅  Thread network saved → {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Threading: 100%|██████████| 113708/113708 [00:13<00:00, 8475.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Thread network saved → /Users/jacksonsorenson/Desktop/r_dabs_network.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build a hierarchical (post › comment › reply) CSV\n",
    "for r/Dabs using the extracted subset files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── 1.  FILE LOCATIONS ───────────────────────────────────────────\n",
    "POSTS_FILE = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:Dabs/Extracted Columns/\"\n",
    "    \"r_dabs_posts_subset.csv\"\n",
    ")\n",
    "COMMENTS_FILE = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:Dabs/Extracted Columns/\"\n",
    "    \"r_dabs_comments_subset.csv\"\n",
    ")\n",
    "OUT_FILE = Path.home() / \"Desktop\" / \"r_dabs_network.csv\"\n",
    "\n",
    "# ── 2.  SMALL HELPERS ────────────────────────────────────────────\n",
    "def strip_id(x: str) -> str:\n",
    "    \"\"\"Remove 't3_' / 't1_' prefixes if present.\"\"\"\n",
    "    return x.split(\"_\", 1)[1] if isinstance(x, str) and \"_\" in x else x\n",
    "\n",
    "def to_int(x):\n",
    "    try: return int(x)\n",
    "    except Exception: return pd.NA\n",
    "\n",
    "# ── 3.  LOAD & NORMALISE ────────────────────────────────────────\n",
    "posts    = pd.read_csv(POSTS_FILE, dtype=str)\n",
    "comments = pd.read_csv(COMMENTS_FILE, dtype=str)\n",
    "\n",
    "posts[\"post_id\"]        = posts[\"id\"].apply(strip_id)\n",
    "posts[\"created_utc_int\"] = posts[\"created_utc\"].apply(to_int)\n",
    "\n",
    "comments[\"comment_id\"]      = comments[\"id\"].apply(strip_id)\n",
    "comments[\"parent_id_norm\"]  = comments[\"parent_id\"].apply(strip_id)\n",
    "comments[\"created_utc_int\"] = comments[\"created_utc\"].apply(to_int)\n",
    "\n",
    "# ── 4.  BUILD CHILD LISTS FOR EACH PARENT ───────────────────────\n",
    "children = {}\n",
    "for _, row in comments.iterrows():\n",
    "    children.setdefault(row[\"parent_id_norm\"], []).append(row)\n",
    "\n",
    "for lst in children.values():                      # chronological within each parent\n",
    "    lst.sort(key=lambda r: r[\"created_utc_int\"] or 0)\n",
    "\n",
    "# ── 5.  QUICK TEXT LOOK-UPS ─────────────────────────────────────\n",
    "post_text    = {r[\"post_id\"]: f\"{r.get('title','')}\\n\\n{r.get('selftext','')}\"\n",
    "                for _, r in posts.iterrows()}\n",
    "comment_text = {r[\"comment_id\"]: r.get(\"body\", \"\")\n",
    "                for _, r in comments.iterrows()}\n",
    "get_parent_txt = lambda pid: comment_text.get(pid) or post_text.get(pid) or \"\"\n",
    "\n",
    "# ── 6.  DEPTH-FIRST WALK TO EMIT ROWS ───────────────────────────\n",
    "rows = []\n",
    "\n",
    "def dfs(parent_id: str, depth: int):\n",
    "    \"\"\"Attach all children of *parent_id* depth-first.\"\"\"\n",
    "    for com in children.get(parent_id, []):\n",
    "        cid   = com[\"comment_id\"]\n",
    "        ntype = \"comment\" if depth == 1 else \"reply\"\n",
    "        rows.append({\n",
    "            \"node_type\"        : ntype,\n",
    "            \"depth\"            : depth,\n",
    "            \"id\"               : cid,\n",
    "            \"parent_id\"        : com[\"parent_id_norm\"],\n",
    "            \"post_id\"          : strip_id(com.get(\"link_id\", \"\")) or \"\",  # may be absent in subset\n",
    "            \"created_utc\"      : com[\"created_utc\"],\n",
    "            \"author\"           : com[\"author\"],\n",
    "            \"score\"            : com[\"score\"],\n",
    "            \"body_text\"        : comment_text[cid],\n",
    "            \"parent_body_text\" : get_parent_txt(com[\"parent_id_norm\"]),\n",
    "            \"distinguished\"    : com.get(\"distinguished\", \"\")\n",
    "        })\n",
    "        dfs(cid, depth + 1)\n",
    "\n",
    "# walk each submission chronologically\n",
    "posts = posts.sort_values(\"created_utc_int\")\n",
    "for _, p in tqdm(posts.iterrows(), total=len(posts), desc=\"Threading\"):\n",
    "    pid = p[\"post_id\"]\n",
    "    rows.append({\n",
    "        \"node_type\"        : \"post\",\n",
    "        \"depth\"            : 0,\n",
    "        \"id\"               : pid,\n",
    "        \"parent_id\"        : \"\",\n",
    "        \"post_id\"          : pid,\n",
    "        \"created_utc\"      : p[\"created_utc\"],\n",
    "        \"author\"           : p[\"author\"],\n",
    "        \"score\"            : p[\"score\"],\n",
    "        \"body_text\"        : post_text[pid],\n",
    "        \"parent_body_text\" : \"\",\n",
    "        \"distinguished\"    : p.get(\"distinguished\", \"\"),\n",
    "        \"num_comments\"     : p.get(\"num_comments\", \"\"),\n",
    "        \"media\"            : p.get(\"media\", \"\")\n",
    "    })\n",
    "    dfs(pid, depth=1)\n",
    "\n",
    "# ── 7.  SAVE OUTPUT ─────────────────────────────────────────────\n",
    "pd.DataFrame(rows).to_csv(OUT_FILE, index=False)\n",
    "print(f\"✅  Thread network saved → {OUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
