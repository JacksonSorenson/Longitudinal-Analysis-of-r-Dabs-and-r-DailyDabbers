{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥  Reading source file â€¦\n",
      "âœ‚ï¸  Selecting relevant columns â€¦\n",
      "ğŸ’¾  Writing trimmed CSV â€¦\n",
      "âœ…  Done! Saved to â†’ /Users/jacksonsorenson/Desktop/r_daily_dabbers_posts_subset.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extract the analysis-ready columns from r_daily_dabbers_posts.csv\n",
    "and write a lighter CSV to the Desktop.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 1. FILE LOCATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SRC  = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:DailyDabbers/r_daily_dabbers_posts.csv\"\n",
    ")\n",
    "DEST = Path.home() / \"Desktop\" / \"r_daily_dabbers_posts_subset.csv\"\n",
    "\n",
    "# â”€â”€ 2. COLUMNS TO KEEP  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Pushshift exports sometimes capital-case these; .str.lower() fixes mismatches.\n",
    "KEEP = {\n",
    "    \"author\",\n",
    "    \"created_utc\",\n",
    "    \"distinguished\",\n",
    "    \"id\",\n",
    "    \"media\",          # Pushshift calls it â€œmediaâ€\n",
    "    \"num_comments\",\n",
    "    \"score\",\n",
    "    \"title\",\n",
    "    \"selftext\",\n",
    "    \"subreddit\",\n",
    "}\n",
    "\n",
    "# â”€â”€ 3. LOAD, FILTER, SAVE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“¥  Reading source file â€¦\")\n",
    "df = pd.read_csv(SRC, dtype=str)         # keep everything as string for safety\n",
    "df.columns = df.columns.str.lower()      # handle Title vs title, Score vs score\n",
    "\n",
    "missing = KEEP - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Column(s) not found in file: {missing}\")\n",
    "\n",
    "print(\"âœ‚ï¸  Selecting relevant columns â€¦\")\n",
    "df_subset = df[list(KEEP)]\n",
    "\n",
    "print(\"ğŸ’¾  Writing trimmed CSV â€¦\")\n",
    "df_subset.to_csv(DEST, index=False)\n",
    "print(f\"âœ…  Done! Saved to â†’ {DEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥  Reading source file â€¦\n",
      "âœ‚ï¸  Selecting relevant columns â€¦\n",
      "ğŸ’¾  Writing trimmed CSV â€¦\n",
      "âœ…  Done! Saved to â†’ /Users/jacksonsorenson/Desktop/r_dailydabbers_comments_subset.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Trim r_dailydabbers_comments copy.csv down to the columns needed for\n",
    "thread-building and analysis, then save the lighter file to the Desktop.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 1. FILE LOCATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SRC  = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:DailyDabbers/\"\n",
    "    \"r_dailydabbers_comments copy.csv\"\n",
    ")\n",
    "DEST = Path.home() / \"Desktop\" / \"r_dailydabbers_comments_subset.csv\"\n",
    "\n",
    "# â”€â”€ 2. COLUMNS TO KEEP  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "KEEP = {\n",
    "    \"author\",\n",
    "    \"body\",\n",
    "    \"created_utc\",\n",
    "    \"id\",\n",
    "    \"parent_id\",\n",
    "    \"score\",\n",
    "    \"distinguished\",\n",
    "    \"subreddit\",\n",
    "}\n",
    "\n",
    "# â”€â”€ 3. LOAD, FILTER, SAVE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“¥  Reading source file â€¦\")\n",
    "df = pd.read_csv(SRC, dtype=str)\n",
    "df.columns = df.columns.str.lower()      # normalise capitalisation (Body â†’ body)\n",
    "\n",
    "missing = KEEP - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Column(s) not found in file: {missing}\")\n",
    "\n",
    "print(\"âœ‚ï¸  Selecting relevant columns â€¦\")\n",
    "df_subset = df[list(KEEP)]\n",
    "\n",
    "print(\"ğŸ’¾  Writing trimmed CSV â€¦\")\n",
    "df_subset.to_csv(DEST, index=False)\n",
    "print(f\"âœ…  Done! Saved to â†’ {DEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥  Reading source file â€¦\n",
      "âœ‚ï¸  Selecting columns in the specified order â€¦\n",
      "ğŸ’¾  Writing trimmed CSV â€¦\n",
      "âœ…  Done! Saved to â†’ /Users/jacksonsorenson/Desktop/r_dabs_posts_subset.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create r_dabs_posts_subset.csv with columns in the order:\n",
    "author, created_utc, distinguished, id, media, num_comments,\n",
    "score, title, selftext, subreddit\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 1. FILE LOCATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SRC  = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:Dabs/r_dabs_posts.csv\"\n",
    ")\n",
    "DEST = Path.home() / \"Desktop\" / \"r_dabs_posts_subset.csv\"\n",
    "\n",
    "# â”€â”€ 2. COLUMNS TO KEEP (IN DESIRED ORDER) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "KEEP_ORDERED = [\n",
    "    \"author\",\n",
    "    \"created_utc\",\n",
    "    \"distinguished\",\n",
    "    \"id\",\n",
    "    \"media\",         # Pushshift calls it â€œmediaâ€\n",
    "    \"num_comments\",\n",
    "    \"score\",\n",
    "    \"title\",\n",
    "    \"selftext\",\n",
    "    \"subreddit\",\n",
    "]\n",
    "\n",
    "# â”€â”€ 3. LOAD â†’ FILTER â†’ SAVE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“¥  Reading source file â€¦\")\n",
    "df = pd.read_csv(SRC, dtype=str)\n",
    "df.columns = df.columns.str.lower()          # handle any capitalisation\n",
    "\n",
    "missing = set(KEEP_ORDERED) - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Column(s) not found in file: {missing}\")\n",
    "\n",
    "print(\"âœ‚ï¸  Selecting columns in the specified order â€¦\")\n",
    "df_subset = df[KEEP_ORDERED]\n",
    "\n",
    "print(\"ğŸ’¾  Writing trimmed CSV â€¦\")\n",
    "df_subset.to_csv(DEST, index=False)\n",
    "print(f\"âœ…  Done! Saved to â†’ {DEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥  Reading source file â€¦\n",
      "âœ‚ï¸  Selecting columns in the specified order â€¦\n",
      "ğŸ’¾  Writing trimmed CSV â€¦\n",
      "âœ…  Done! Saved to â†’ /Users/jacksonsorenson/Desktop/r_dabs_comments_subset.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Create r_dabs_comments_subset.csv with columns in the order:\n",
    "author, body, created_utc, id, parent_id, score,\n",
    "distinguished, subreddit\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 1. FILE LOCATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SRC  = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:Dabs/rdabs_comments.csv\"\n",
    ")\n",
    "DEST = Path.home() / \"Desktop\" / \"r_dabs_comments_subset.csv\"\n",
    "\n",
    "# â”€â”€ 2. COLUMNS TO KEEP (IN DESIRED ORDER) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "KEEP_ORDERED = [\n",
    "    \"author\",        # commenter username\n",
    "    \"body\",          # text of the comment\n",
    "    \"created_utc\",   # UTC timestamp\n",
    "    \"id\",            # unique comment ID\n",
    "    \"parent_id\",     # ID of parent (post or comment)\n",
    "    \"score\",         # up-vote score\n",
    "    \"distinguished\", # moderator/admin flag\n",
    "    \"subreddit\",     # should be \"Dabs\"\n",
    "]\n",
    "\n",
    "# â”€â”€ 3. LOAD â†’ FILTER â†’ SAVE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“¥  Reading source file â€¦\")\n",
    "df = pd.read_csv(SRC, dtype=str)\n",
    "df.columns = df.columns.str.lower()          # normalise capitalisation\n",
    "\n",
    "missing = set(KEEP_ORDERED) - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Column(s) not found in file: {missing}\")\n",
    "\n",
    "print(\"âœ‚ï¸  Selecting columns in the specified order â€¦\")\n",
    "df_subset = df[KEEP_ORDERED]\n",
    "\n",
    "print(\"ğŸ’¾  Writing trimmed CSV â€¦\")\n",
    "df_subset.to_csv(DEST, index=False)\n",
    "print(f\"âœ…  Done! Saved to â†’ {DEST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…  overlap saved â†’ /Users/jacksonsorenson/Desktop/overlap_users_network.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build overlap_users_network.csv where the first column is `author`.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€ 1. FILE PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DABS_NET = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:Dabs/Combined Dabs W Network/\"\n",
    "    \"r_dabs_network.csv\"\n",
    ")\n",
    "DAILY_NET = Path(\n",
    "    \"/Users/jacksonsorenson/Documents/Computational Media Lab/Weed Study/\"\n",
    "    \"CSV for mass database organization/R:DailyDabbers/Combined Daily Dabbers W Network/\"\n",
    "    \"r_dailydabbers_network.csv\"\n",
    ")\n",
    "OUT = Path.home() / \"Desktop\" / \"overlap_users_network.csv\"\n",
    "\n",
    "# â”€â”€ 2. LOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dabs   = pd.read_csv(DABS_NET,  dtype=str)\n",
    "daily  = pd.read_csv(DAILY_NET, dtype=str)\n",
    "dabs.columns  = dabs.columns.str.lower()\n",
    "daily.columns = daily.columns.str.lower()\n",
    "\n",
    "# â”€â”€ 3.  OVERLAP USERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "overlap = set(dabs[\"author\"].dropna()) & set(daily[\"author\"].dropna())\n",
    "dabs  = dabs[dabs[\"author\"].isin(overlap)].copy()\n",
    "daily = daily[daily[\"author\"].isin(overlap)].copy()\n",
    "dabs[\"source_sub\"]  = \"Dabs\"\n",
    "daily[\"source_sub\"] = \"DailyDabbers\"\n",
    "\n",
    "# â”€â”€ 4.  COMBINE & SORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.concat([dabs, daily], ignore_index=True)\n",
    "df[\"created_utc_int\"] = pd.to_numeric(df[\"created_utc\"], errors=\"coerce\")\n",
    "df.sort_values([\"author\", \"created_utc_int\"], inplace=True)\n",
    "\n",
    "# â”€â”€ 5.  REORDER SO AUTHOR IS FIRST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cols = [\"author\"] + [c for c in df.columns if c != \"author\"]\n",
    "df = df[cols]\n",
    "\n",
    "# â”€â”€ 6.  SAVE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df.to_csv(OUT, index=False)\n",
    "print(f\"âœ…  overlap saved â†’ {OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
